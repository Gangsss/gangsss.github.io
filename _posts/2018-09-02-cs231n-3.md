---
title: "CS231n-3강"
subtitle: "CS231n"
categories: data
tags: cs231n
comments: true
---



To do list

- Define Loss function
- Optimiztion - finding parameters that minimize the loss function



Loss

- Hinge loss(SVM)
- Cross entropy loss(Softmax)



SVM(Hinge loss)

![31006647-76e12822-a538-11e7-85c2-5e9e6fd81b32](C:\Users\Kim\Desktop\31006647-76e12822-a538-11e7-85c2-5e9e6fd81b32.JPG)

Ex) 고양이 이미지 데이터를 input이라고 하면



Sj( a라고 하겠습니다)는 잘못된 라벨에 대한 점수(차,개구리 라벨에 대한 점수)



Syi( b라고 하겠습니다)는 제대로 된 라벨에 대한 점수(고양이 라벨에 대한 점수)



1은 safety  margin



loss 값은 결국

고양이, 강이지, 차를 분류하는 예로 들면



고양이의 loss 값은

강아지와 차의 loss값의 합입니다.



- Regularization

  Loss 값이 최소면 좋을까(X)

  Loss값이 최소로 하는 weight값이 uniqe해야 한다.



​	강의에서는 밑에와 같이 표현한다.

​	Trading off trainging loss and genrealization loss on test set

​	training 데이터에 정확도는 낮아질테지만 test 데이터에 대한 performance는 나아질 것





Softmax Classifier(Multinomial Logistic Regression)

![992B1B3359E5E3DE05](C:\Users\Kim\Desktop\992B1B3359E5E3DE05.png)



정확한 클래스의 -log클래스 최소화 하는게 목표 

 log값 취하는 이유는 수학적으로 나이스하기때문



Softmax loss(cross entropy) 값 구하는 순서

1. 정규화하지 않은 인풋에 대한 P값
2. 이를 exp로 시키고 
3. 정규화 후
4. -log를 취한다



Tip)

W의 초기 값을 0과 값은 값으로

-> score가 0에 가까울 것

->exp(0)= 1로 다 같고 확률로 나타내면 1/3

-> - log(1/3) (단, 클래스가 3일 때 n이면 -log(1/n))

-> 학습전 sanity check로 이용



SVM vs Softmax

SVM : 데이터에 민감하지 않고 둔감함

Softmax : 모든 데이터들을 고려하므로 민감함



Optimization

각각의 loss 값이 있을 때 로스 값은 data와는 상관 없이 W만의 함수

1) random search : 절대 하면 안된다.

2) numerical gradient : 수식을 통해 미분 값을 계산, 기울기를 구해 최적화 - 너무 느리다

3) analytic gradient : 실제로 많이 쓰이는 것, 빠르고 정확/ 중간에 numerical gradient을 통해 check



mini batch 

- training 셋 일부만 이용
- gradient 계산, parameter 업데이트 후 다른 미니 베치 이용해  반복
- 32,64,128개 사이즈 gpu 상태에 따라 분배에서 사용

