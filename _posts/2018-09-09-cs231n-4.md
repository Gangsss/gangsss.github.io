---
layout: post
title: "CS231n- 4강"
subtitle: "CS231n"
categories: CS231n
tags: CS231n
comments: true
---

from http://cs231n.stanford.edu/

## CS231n

##  4) Back Propagation and Neural Networks

### computational graph 

: 각 연산을 node로 나타내서 복잡한 연산의 모든 절차를 그래프로 표현하는 방법 

### chain rule



df/dx = df/dq * dq/dx   



Add gate :gradient distrubutor

- 앞 노드에 gradient 전달/ upstream 동등히 배분



Q : max gate?

A : gradient router (gradient/0)

Q: mul gate?

A: gradient switcher(둘이 바꿔준다)



- input이 vector라면?

  local gradient - Jacobian matrix

  - derivative of each element of z w.r.t each element of x )
  - [벡터 미적분학](https://ko.wikipedia.org/wiki/%EB%B2%A1%ED%84%B0_%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99)에서, **야코비 행렬**([영어](https://ko.wikipedia.org/wiki/%EC%98%81%EC%96%B4): Jacobian matrix)은 [다변수](https://ko.wikipedia.org/wiki/%EB%8B%A4%EB%B3%80%EC%88%98_%ED%95%A8%EC%88%98) [벡터 함수](https://ko.wikipedia.org/wiki/%EB%B2%A1%ED%84%B0_%ED%95%A8%EC%88%98)의 [도함수](https://ko.wikipedia.org/wiki/%EB%8F%84%ED%95%A8%EC%88%98) [행렬](https://ko.wikipedia.org/wiki/%ED%96%89%EB%A0%AC)이다. **야코비 행렬식**([영어](https://ko.wikipedia.org/wiki/%EC%98%81%EC%96%B4): Jacobian determinant)은 야코비 행렬의 [행렬식](https://ko.wikipedia.org/wiki/%ED%96%89%EB%A0%AC%EC%8B%9D)이다. (https://ko.wikipedia.org/wiki/%EC%95%BC%EC%BD%94%EB%B9%84_%ED%96%89%EB%A0%AC)



Q. 4096 dimention의 output vector jacobian matrix's size?

A. 4096 * 4096

Q. size 100의 minibatch를 사용하면 Jacobian matrix의 사이즈는?

A. 409600 * 409600 - 더 큰 크기의 행렬이 된다.

Q. 첫 번째 질문 상황일 때 Jacobian matrix는 어떻게 생겼는가?

A :diagonal( 대각행렬)



Summary

![31547837-c7c5cafa-b063-11e7-9d62-c9e01de18b09](C:\Users\Kim\Desktop\gangsss.github.io\assets\img\cs231n-4-1)



Nueral Network



Before

Linear score function : f=Wx 



Now (activiation function)

2-layer Neural Network : f=W2 max(0,W1x)

 



 

